{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train network for Gesture Recognition from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import ModelDefinition\n",
    "from dataset import ReadImages, collection\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readFrameAnnotation(annotationFile):\n",
    "    \"\"\"\n",
    "        read annotation file\n",
    "        return the list of annotation ([start, end], gesture)\n",
    "    \"\"\"\n",
    "    anno = []\n",
    "    for l in open(annotationFile).read().splitlines():\n",
    "        s = l.split(' ')\n",
    "        anno += [ ([int(s[1]), int(s[2])], int(s[0])-1)]\n",
    "    return anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findGestureFrame(frameNumber, annotationFile):\n",
    "    \"\"\"\n",
    "        from Frame Number and the list of annotation\n",
    "        return the Gesture or None if not in annation\n",
    "    \"\"\"\n",
    "    for seq, gest in annotationFile:\n",
    "        if frameNumber >= seq[0] and frameNumber <= seq[1]:\n",
    "            return gest\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copyParameters(net, netBase):\n",
    "    for i, f in enumerate(net.features):\n",
    "        if type(f) is torch.nn.modules.conv.Conv2d:\n",
    "            if i < len(netBase.features._modules):\n",
    "                if f.weight.size() == netBase.features[i].weight.size():\n",
    "                    f.weight.data = netBase.features[i].weight.data\n",
    "                    f.bias.data = netBase.features[i].bias.data\n",
    "    for i, c in enumerate(net.classifier):\n",
    "        if type(c) is torch.nn.modules.linear.Linear:\n",
    "            if c.weight.size() == netBase.classifier[i].weight.size():\n",
    "                c.weight.data = netBase.classifier[i].weight.data\n",
    "                c.bias.data = netBase.classifier[i].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fillInput(nframe, video, with_cuda=False):\n",
    "    t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "    if with_cuda:\n",
    "        inputs = torch.Tensor(nframe,3,225,225).cuda()\n",
    "    else:\n",
    "        inputs = torch.Tensor(nframe,3,225,225)\n",
    "    for j in range(nframe):\n",
    "        ret, frame = video.read()\n",
    "        if frame is None:\n",
    "            print(\"Error : None Frame\")\n",
    "            exit(0)\n",
    "        frame = t(frame)\n",
    "        inputs[j] = frame\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO : test if difference between learning only gesture per batch\n",
    "\n",
    "def learnSequence(sequence, gesture, video, model, criterion, optimize, batchSize=32):\n",
    "    numberFrame = seq[1] - seq[0]\n",
    "    running_loss = 0\n",
    "    while numberFrame > 0:\n",
    "        if numberFrame >= batchSize:\n",
    "            inputs = fillInput(batchSize, video, True)\n",
    "            numberFrame -= batchSize\n",
    "            \n",
    "            labels = torch.LongTensor([gesture]*batchSize).cuda()\n",
    "        else:\n",
    "            inputs = fillInput(numberFrame, video, True)\n",
    "            labels = torch.LongTensor([gesture]*numberFrame).cuda()\n",
    "            numberFrame = 0\n",
    "            \n",
    "        inputs = Variable(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, Variable(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testSequence(seq, gesture, video, model, batchSize=32):\n",
    "    numberFrame = seq[1] - seq[0]\n",
    "    correct = 0\n",
    "    while numberFrame > 0:\n",
    "        if numberFrame >= batchSize:\n",
    "            inputs = fillInput(batchSize, video, True)\n",
    "            numberFrame -= batchSize\n",
    "        else:\n",
    "            inputs = fillInput(numberFrame, video, True)\n",
    "            numberFrame = 0\n",
    "            \n",
    "        inputs = Variable(inputs)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.tolist()\n",
    "        for i in range(len(predicted)):\n",
    "            correct = (predicted[i][0] == gesture) or correct\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testNet(model, testDir):\n",
    "    model.eval()\n",
    "    t = 0\n",
    "    c = 0\n",
    "    for video in glob.glob(testDir+'*.mp4'):\n",
    "        print(\"Test video \", video)\n",
    "        fName = path.splitext(path.basename(video))[0] #basename\n",
    "        annotation = readFrameAnnotation(testDir+fName)\n",
    "        videoCap = cv2.VideoCapture(video)\n",
    "        for seq, gesture in annotation:\n",
    "            #print(\"Frame : \", seq[0], '-', seq[1])\n",
    "            #t += seq[1] - seq[0]\n",
    "            t += 1\n",
    "            c += testSequence(seq, gesture, videoCap, model)\n",
    "            #print(\"Correct :\", c)\n",
    "    print(\"Correctness : \", c, '/', t)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testNFrame(model, testDir, frame_num=5):\n",
    "    \"\"\"\n",
    "        Test the model with a window of frame_num frames\n",
    "    \"\"\"\n",
    "    print(\"TestNFrame\")\n",
    "    model.eval()\n",
    "    c = 0\n",
    "    t = 0\n",
    "    for video in glob.glob(testDir+'*.mp4'):\n",
    "        print(\"Test video \", video)\n",
    "        fName = path.splitext(path.basename(video))[0] #basename\n",
    "        annotation = readFrameAnnotation(testDir+fName)\n",
    "        videoCap = cv2.VideoCapture(video)\n",
    "        for seq, gesture in annotation:\n",
    "            for i in range( (seq[1]-seq[0])/frame_num):\n",
    "                inputs = fillInput(frame_num, videoCap, True)\n",
    "                inputs = Variable(inputs, volatile=True)\n",
    "                outputs = model(inputs)\n",
    "                t += 1\n",
    "                c += (int(outputs.data.sum(0).max(1)[1].cpu()[0][0]) == gesture)\n",
    "    print(\"Correctness : \", c, '/', t)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testImages(model, testDir, transf=transforms.ToTensor(), batch_size=32):\n",
    "    \"\"\"\n",
    "        Test model on images organized : class/imName\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dtest = datasets.ImageFolder(testDir, transform=transf)\n",
    "    l = torch.utils.data.DataLoader(dtest, batch_size=batch_size, num_workers=6, drop_last=False, pin_memory=False)\n",
    "    c = 0\n",
    "    for batch_idx, (data,target) in enumerate(l):\n",
    "        data = Variable(data.cuda(), volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1].cpu()\n",
    "        c += pred.eq(target).sum()\n",
    "    print(\"Correctness on Images : \", c,\"/\", len(l.dataset), ':', float(c)/len(l.dataset)*100, '%' )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlexNetS(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNetS, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            #nn.Conv2d(256, , kernel_size=13, padding=1),\n",
    "            #nn.Conv2d(256, 256, kernel_size=13, padding=1),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(13,13),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.newClassifier = nn.Sequential(\n",
    "            nn.Linear(256,6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = x.view(x.size(0), 256)\n",
    "        x = self.newClassifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       " -3.9006 -4.9893  1.0369 -0.3465  3.7769  3.7546\n",
       "[torch.FloatTensor of size 32x6]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = AlexNetS()\n",
    "t = Variable(torch.Tensor(32,3,225,225))\n",
    "m(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainOnVideos():\n",
    "    rootDir = '/video/Gesture/'\n",
    "    model = AlexNetS()\n",
    "    #model = models.VGG(models.vgg.make_layers(models.vgg.cfg['B'], batch_norm=True), num_classes=6)\n",
    "    copyParameters(model, models.alexnet(pretrained=True))\n",
    "    #model = torch.load('best-model.ckpt')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.01\n",
    "\n",
    "    model.cuda()\n",
    "    best = testNet(model, rootDir+'test/')\n",
    "    rl = 0\n",
    "    videos = glob.glob(rootDir+'*.mp4')\n",
    "    j = 0\n",
    "    for epoch in range(5):\n",
    "        random.shuffle(videos)\n",
    "        for video in videos:\n",
    "            model.train()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "            #testNet(model, rootDir+'test/')\n",
    "            print(\"Video \", video)\n",
    "            fName = path.splitext(path.basename(video))[0] #basename\n",
    "            annotation = readFrameAnnotation(rootDir+'annotation/'+fName) #read annotation\n",
    "\n",
    "            videoCap = cv2.VideoCapture(video)\n",
    "            i = 0\n",
    "            for seq, gesture in annotation:\n",
    "                #print(\"Sequence \", seq, \" Gesture : \", gesture)\n",
    "                rl += learnSequence(seq, gesture, videoCap, model, criterion, optimizer)\n",
    "                i += 1\n",
    "                if i%5 == 4:\n",
    "                    print(\"[epoch %d] loss : %.3f\" % (epoch, rl/5) )\n",
    "                    #i = 0\n",
    "                    rl = 0.0\n",
    "            \n",
    "            if j%5 == 4:\n",
    "                r = testNet(model, rootDir+'test/')\n",
    "                if r > best:\n",
    "                    best = r\n",
    "                    print(\"Saving best model\")\n",
    "                    torch.save(model, 'best-model.ckpt')\n",
    "            #torch.save(model, path.join('model-'+str(epoch)+\".ckpt\"))\n",
    "            #lr = 0.001\n",
    "            j += 1\n",
    "            videoCap.release()\n",
    "        lr = 0.001\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainOnImages(model, rootDir='/video/GestureImages/trainBGIMAG3_All/', batch_size=32, trainTrans=transforms.ToTensor(), testTrans=transforms.ToTensor(), lr=0.001, epoch=50):\n",
    "    d = datasets.ImageFolder(rootDir, transform=trainTrans)\n",
    "    l = torch.utils.data.DataLoader(d, batch_size=32, shuffle=True, num_workers=6, drop_last=True, pin_memory=False)\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    best = testImages(model, '/video/GestureImages/trainBGOffice_All/')\n",
    "    #testNet(model=model)\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data,target) in enumerate(l):\n",
    "            #data, target = data.cuda(device=0), target.cuda(device=0)\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(l.dataset),\n",
    "                    100. * batch_idx / len(l), loss.data[0]))\n",
    "        #r = testNFrame(model, '/video/Gesture/test/')\n",
    "        r = testImages(model, '/video/GestureImages/trainBGOffice_All/')\n",
    "        if r > best :\n",
    "            best = r\n",
    "            print(\"Saving best model\")\n",
    "            torch.save(model, 'best-model.ckpt')\n",
    "        if ep%2 == 0:\n",
    "            testNet(model=model, testDir='/video/Gesture/test/')\n",
    "            testNFrame(model, '/video/Gesture/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness on Images :  5144 / 7958 : 64.6393566223 %\n",
      "Train Epoch: 0 [0/7641 (0%)]\tLoss: 0.423641\n",
      "Train Epoch: 0 [1600/7641 (21%)]\tLoss: 0.417205\n",
      "Train Epoch: 0 [3200/7641 (42%)]\tLoss: 0.567071\n",
      "Train Epoch: 0 [4800/7641 (63%)]\tLoss: 0.539175\n",
      "Train Epoch: 0 [6400/7641 (84%)]\tLoss: 0.220800\n",
      "Correctness on Images :  5681 / 7958 : 71.387283237 %\n",
      "Saving best model\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  8 / 42\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  14 / 165\n",
      "Train Epoch: 1 [0/7641 (0%)]\tLoss: 0.267504\n",
      "Train Epoch: 1 [1600/7641 (21%)]\tLoss: 0.327193\n",
      "Train Epoch: 1 [3200/7641 (42%)]\tLoss: 0.734355\n",
      "Train Epoch: 1 [4800/7641 (63%)]\tLoss: 0.215852\n",
      "Train Epoch: 1 [6400/7641 (84%)]\tLoss: 0.310971\n",
      "Correctness on Images :  5477 / 7958 : 68.8238250817 %\n",
      "Train Epoch: 2 [0/7641 (0%)]\tLoss: 0.103206\n",
      "Train Epoch: 2 [1600/7641 (21%)]\tLoss: 0.213137\n",
      "Train Epoch: 2 [3200/7641 (42%)]\tLoss: 0.192684\n",
      "Train Epoch: 2 [4800/7641 (63%)]\tLoss: 0.115641\n",
      "Train Epoch: 2 [6400/7641 (84%)]\tLoss: 0.314189\n",
      "Correctness on Images :  6123 / 7958 : 76.9414425735 %\n",
      "Saving best model\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  9 / 42\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  15 / 165\n",
      "Train Epoch: 3 [0/7641 (0%)]\tLoss: 0.145376\n",
      "Train Epoch: 3 [1600/7641 (21%)]\tLoss: 0.294291\n",
      "Train Epoch: 3 [3200/7641 (42%)]\tLoss: 0.414334\n",
      "Train Epoch: 3 [4800/7641 (63%)]\tLoss: 0.229566\n",
      "Train Epoch: 3 [6400/7641 (84%)]\tLoss: 0.195932\n",
      "Correctness on Images :  5906 / 7958 : 74.2146267907 %\n",
      "Train Epoch: 4 [0/7641 (0%)]\tLoss: 0.090618\n",
      "Train Epoch: 4 [1600/7641 (21%)]\tLoss: 0.167217\n",
      "Train Epoch: 4 [3200/7641 (42%)]\tLoss: 0.226192\n",
      "Train Epoch: 4 [4800/7641 (63%)]\tLoss: 0.077010\n",
      "Train Epoch: 4 [6400/7641 (84%)]\tLoss: 0.403833\n",
      "Correctness on Images :  5282 / 7958 : 66.3734606685 %\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  9 / 42\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  18 / 165\n",
      "Train Epoch: 5 [0/7641 (0%)]\tLoss: 0.057186\n",
      "Train Epoch: 5 [1600/7641 (21%)]\tLoss: 0.079571\n",
      "Train Epoch: 5 [3200/7641 (42%)]\tLoss: 0.333354\n",
      "Train Epoch: 5 [4800/7641 (63%)]\tLoss: 0.330907\n",
      "Train Epoch: 5 [6400/7641 (84%)]\tLoss: 0.084393\n",
      "Correctness on Images :  5568 / 7958 : 69.9673284745 %\n",
      "Train Epoch: 6 [0/7641 (0%)]\tLoss: 0.028055\n",
      "Train Epoch: 6 [1600/7641 (21%)]\tLoss: 0.130746\n",
      "Train Epoch: 6 [3200/7641 (42%)]\tLoss: 0.055652\n",
      "Train Epoch: 6 [4800/7641 (63%)]\tLoss: 0.034838\n",
      "Train Epoch: 6 [6400/7641 (84%)]\tLoss: 0.099892\n",
      "Correctness on Images :  5584 / 7958 : 70.1683840161 %\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  9 / 42\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n",
      "Correctness :  15 / 165\n",
      "Train Epoch: 7 [0/7641 (0%)]\tLoss: 0.254421\n",
      "Train Epoch: 7 [1600/7641 (21%)]\tLoss: 0.233564\n",
      "Train Epoch: 7 [3200/7641 (42%)]\tLoss: 0.072347\n",
      "Train Epoch: 7 [4800/7641 (63%)]\tLoss: 0.100077\n",
      "Train Epoch: 7 [6400/7641 (84%)]\tLoss: 0.105993\n",
      "Correctness on Images :  5745 / 7958 : 72.1915054034 %\n",
      "Train Epoch: 8 [0/7641 (0%)]\tLoss: 0.067360\n",
      "Train Epoch: 8 [1600/7641 (21%)]\tLoss: 0.121401\n",
      "Train Epoch: 8 [3200/7641 (42%)]\tLoss: 0.134902\n",
      "Train Epoch: 8 [4800/7641 (63%)]\tLoss: 0.072190\n",
      "Train Epoch: 8 [6400/7641 (84%)]\tLoss: 0.032089\n",
      "Correctness on Images :  6228 / 7958 : 78.2608695652 %\n",
      "Saving best model\n",
      "Test video  /video/Gesture/test/v43.mp4\n",
      "Test video  /video/Gesture/test/u19.mp4\n"
     ]
    }
   ],
   "source": [
    "#model = AlexNetS()\n",
    "#copyParameters(model, models.alexnet(pretrained=True))\n",
    "t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "model=torch.load('best-model.ckpt')\n",
    "trainOnImages(model, epoch=10)\n",
    "\n",
    "trainOnImages(model, '/video/GestureImages/trainBGOffice_All/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
