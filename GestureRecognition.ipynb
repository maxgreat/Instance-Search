{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train network for Gesture Recognition from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import ModelDefinition\n",
    "from dataset import ReadImages, collection\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readFrameAnnotation(annotationFile):\n",
    "    \"\"\"\n",
    "        read annotation file\n",
    "        return the list of annotation ([start, end], gesture)\n",
    "    \"\"\"\n",
    "    anno = []\n",
    "    for l in open(annotationFile).read().splitlines():\n",
    "        s = l.split(' ')\n",
    "        anno += [ ([int(s[1]), int(s[2])], int(s[0])-1)]\n",
    "    return anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findGestureFrame(frameNumber, annotationFile):\n",
    "    \"\"\"\n",
    "        from Frame Number and the list of annotation\n",
    "        return the Gesture or None if not in annation\n",
    "    \"\"\"\n",
    "    for seq, gest in annotationFile:\n",
    "        if frameNumber >= seq[0] and frameNumber <= seq[1]:\n",
    "            return gest\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copyParameters(net, netBase):\n",
    "    for i, f in enumerate(net.features):\n",
    "        if type(f) is torch.nn.modules.conv.Conv2d:\n",
    "            if i < len(netBase.features._modules):\n",
    "                if f.weight.size() == netBase.features[i].weight.size():\n",
    "                    f.weight.data = netBase.features[i].weight.data\n",
    "                    f.bias.data = netBase.features[i].bias.data\n",
    "    for i, c in enumerate(net.classifier):\n",
    "        if type(c) is torch.nn.modules.linear.Linear:\n",
    "            if c.weight.size() == netBase.classifier[i].weight.size():\n",
    "                c.weight.data = netBase.classifier[i].weight.data\n",
    "                c.bias.data = netBase.classifier[i].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fillInput(nframe, video, with_cuda=False):\n",
    "    t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "    if with_cuda:\n",
    "        inputs = torch.Tensor(nframe,3,225,225).cuda()\n",
    "    else:\n",
    "        inputs = torch.Tensor(nframe,3,225,225)\n",
    "    for j in range(nframe):\n",
    "        ret, frame = video.read()\n",
    "        if frame is None:\n",
    "            print(\"Error : None Frame\")\n",
    "            exit(0)\n",
    "        frame = t(frame)\n",
    "        inputs[j] = frame\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO : test if difference between learning only gesture per batch\n",
    "\n",
    "def learnSequence(sequence, gesture, video, model, criterion, optimize, batchSize=32):\n",
    "    numberFrame = seq[1] - seq[0]\n",
    "    running_loss = 0\n",
    "    while numberFrame > 0:\n",
    "        if numberFrame >= batchSize:\n",
    "            inputs = fillInput(batchSize, video, True)\n",
    "            numberFrame -= batchSize\n",
    "            \n",
    "            labels = torch.LongTensor([gesture]*batchSize).cuda()\n",
    "        else:\n",
    "            inputs = fillInput(numberFrame, video, True)\n",
    "            labels = torch.LongTensor([gesture]*numberFrame).cuda()\n",
    "            numberFrame = 0\n",
    "            \n",
    "        inputs = Variable(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, Variable(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testSequence(seq, gesture, video, model, batchSize=32):\n",
    "    numberFrame = seq[1] - seq[0]\n",
    "    correct = 0\n",
    "    while numberFrame > 0:\n",
    "        if numberFrame >= batchSize:\n",
    "            inputs = fillInput(batchSize, video, True)\n",
    "            numberFrame -= batchSize\n",
    "        else:\n",
    "            inputs = fillInput(numberFrame, video, True)\n",
    "            numberFrame = 0\n",
    "            \n",
    "        inputs = Variable(inputs)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.tolist()\n",
    "        for i in range(len(predicted)):\n",
    "            correct = (predicted[i][0] == gesture) or correct\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testNet(model, testDir):\n",
    "    model.eval()\n",
    "    t = 0\n",
    "    c = 0\n",
    "    for video in glob.glob(testDir+'*.mp4'):\n",
    "        print(\"Test video \", video)\n",
    "        fName = path.splitext(path.basename(video))[0] #basename\n",
    "        annotation = readFrameAnnotation(testDir+fName)\n",
    "        videoCap = cv2.VideoCapture(video)\n",
    "        for seq, gesture in annotation:\n",
    "            #print(\"Frame : \", seq[0], '-', seq[1])\n",
    "            #t += seq[1] - seq[0]\n",
    "            t += 1\n",
    "            c += testSequence(seq, gesture, videoCap, model)\n",
    "            #print(\"Correct :\", c)\n",
    "    print(\"Correctness : \", c, '/', t)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testNFrame(model, testDir, frame_num=5):\n",
    "    \"\"\"\n",
    "        Test the model with a window of frame_num frames\n",
    "    \"\"\"\n",
    "    print(\"TestNFrame\")\n",
    "    model.eval()\n",
    "    c = 0\n",
    "    t = 0\n",
    "    for video in glob.glob(testDir+'*.mp4'):\n",
    "        print(\"Test video \", video)\n",
    "        fName = path.splitext(path.basename(video))[0] #basename\n",
    "        annotation = readFrameAnnotation(testDir+fName)\n",
    "        videoCap = cv2.VideoCapture(video)\n",
    "        for seq, gesture in annotation:\n",
    "            for i in range( (seq[1]-seq[0])/frame_num):\n",
    "                inputs = fillInput(frame_num, videoCap, True)\n",
    "                inputs = Variable(inputs, volatile=True)\n",
    "                outputs = model(inputs)\n",
    "                t += 1\n",
    "                c += (int(outputs.data.sum(0).max(1)[1].cpu()[0][0]) == gesture)\n",
    "    print(\"Correctness : \", c, '/', t)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testImages(model, testDir, transf=transforms.ToTensor(), batch_size=32):\n",
    "    \"\"\"\n",
    "        Test model on images organized : class/imName\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dtest = datasets.ImageFolder(testDir, transform=transf)\n",
    "    l = torch.utils.data.DataLoader(dtest, batch_size=batch_size, num_workers=6, drop_last=False, pin_memory=False)\n",
    "    c = 0\n",
    "    for batch_idx, (data,target) in enumerate(l):\n",
    "        data = Variable(data.cuda(), volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1].cpu()\n",
    "        c += pred.eq(target).sum()\n",
    "    print(\"Correctness on Images : \", c,\"/\", len(l.dataset), ':', float(c)/len(l.dataset)*100, '%' )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AlexNetS(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNetS, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            #nn.Conv2d(256, , kernel_size=13, padding=1),\n",
    "            #nn.Conv2d(256, 256, kernel_size=13, padding=1),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(13,13),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.newClassifier = nn.Sequential(\n",
    "            nn.Linear(256,6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = x.view(x.size(0), 256)\n",
    "        x = self.newClassifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = AlexNetS()\n",
    "t = Variable(torch.Tensor(32,3,225,225))\n",
    "m(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainOnVideos():\n",
    "    rootDir = '/video/Gesture/'\n",
    "    model = AlexNetS()\n",
    "    #model = models.VGG(models.vgg.make_layers(models.vgg.cfg['B'], batch_norm=True), num_classes=6)\n",
    "    copyParameters(model, models.alexnet(pretrained=True))\n",
    "    #model = torch.load('best-model.ckpt')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.01\n",
    "\n",
    "    model.cuda()\n",
    "    best = testNet(model, rootDir+'test/')\n",
    "    rl = 0\n",
    "    videos = glob.glob(rootDir+'*.mp4')\n",
    "    j = 0\n",
    "    for epoch in range(5):\n",
    "        random.shuffle(videos)\n",
    "        for video in videos:\n",
    "            model.train()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "            #testNet(model, rootDir+'test/')\n",
    "            print(\"Video \", video)\n",
    "            fName = path.splitext(path.basename(video))[0] #basename\n",
    "            annotation = readFrameAnnotation(rootDir+'annotation/'+fName) #read annotation\n",
    "\n",
    "            videoCap = cv2.VideoCapture(video)\n",
    "            i = 0\n",
    "            for seq, gesture in annotation:\n",
    "                #print(\"Sequence \", seq, \" Gesture : \", gesture)\n",
    "                rl += learnSequence(seq, gesture, videoCap, model, criterion, optimizer)\n",
    "                i += 1\n",
    "                if i%5 == 4:\n",
    "                    print(\"[epoch %d] loss : %.3f\" % (epoch, rl/5) )\n",
    "                    #i = 0\n",
    "                    rl = 0.0\n",
    "            \n",
    "            if j%5 == 4:\n",
    "                r = testNet(model, rootDir+'test/')\n",
    "                if r > best:\n",
    "                    best = r\n",
    "                    print(\"Saving best model\")\n",
    "                    torch.save(model, 'best-model.ckpt')\n",
    "            #torch.save(model, path.join('model-'+str(epoch)+\".ckpt\"))\n",
    "            #lr = 0.001\n",
    "            j += 1\n",
    "            videoCap.release()\n",
    "        lr = 0.001\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trainOnImages(model, rootDir='/video/GestureImages/trainBGIMAG3_All/', batch_size=32, trainTrans=transforms.ToTensor(), testTrans=transforms.ToTensor(), lr=0.001, epoch=50):\n",
    "    d = datasets.ImageFolder(rootDir, transform=trainTrans)\n",
    "    l = torch.utils.data.DataLoader(d, batch_size=32, shuffle=True, num_workers=6, drop_last=True, pin_memory=False)\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    best = testImages(model, '/video/GestureImages/trainBGOffice_All/')\n",
    "    #testNet(model=model)\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data,target) in enumerate(l):\n",
    "            #data, target = data.cuda(device=0), target.cuda(device=0)\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(l.dataset),\n",
    "                    100. * batch_idx / len(l), loss.data[0]))\n",
    "        #r = testNFrame(model, '/video/Gesture/test/')\n",
    "        r = testImages(model, '/video/GestureImages/trainBGOffice_All/')\n",
    "        if r > best :\n",
    "            best = r\n",
    "            print(\"Saving best model\")\n",
    "            torch.save(model, 'best-model.ckpt')\n",
    "        if ep%2 == 0:\n",
    "            testNet(model=model, testDir='/video/Gesture/test/')\n",
    "            testNFrame(model, '/video/Gesture/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export \n",
    "import torch\n",
    "import torch.onnx\n",
    "from torch.autograd import Variable\n",
    "d = Variable(torch.Tensor(1,3,224,224))\n",
    "model=torch.load('best-model.ckpt').cpu()\n",
    "torch.onnx.export(model, d, \"gesture.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = AlexNetS()\n",
    "#copyParameters(model, models.alexnet(pretrained=True))\n",
    "t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "model=torch.load('best-model.ckpt')\n",
    "trainOnImages(model, epoch=10)\n",
    "\n",
    "trainOnImages(model, '/video/GestureImages/trainBGOffice_All/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## LSTM ##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.k = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.w_i = nn.Parameter(torch.Tensor(4*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.w_h = nn.Parameter(torch.Tensor(4*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(3*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        # TODO include bias terms\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels * self.k * self.k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        \n",
    "        self.w_i.data.uniform_(-stdv, stdv)\n",
    "        self.w_h.data.uniform_(-stdv, stdv)\n",
    "        self.w_c.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x, hx):\n",
    "        h, c = hx\n",
    "        wx = F.conv2d(x, self.w_i, padding=self.padding, stride=self.stride)\n",
    "        wh = F.conv2d(h, self.w_h, padding=self.padding, stride=self.stride)\n",
    "        wc = F.conv2d(c, self.w_c, padding=self.padding, stride=self.stride)\n",
    "        \n",
    "        i = F.sigmoid(wx[:, :self.out_channels] + wh[:, :self.out_channels] + wc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wx[:, self.out_channels:2*self.out_channels] + wh[:, self.out_channels:2*self.out_channels] \n",
    "                + wc[:, self.out_channels:2*self.out_channels])\n",
    "        g = F.tanh(wx[:, 2*self.out_channels:3*self.out_channels] + wh[:, 2*self.out_channels:3*self.out_channels])\n",
    "        \n",
    "        c_t = f * c + i * g\n",
    "        o_t = F.sigmoid(wx[:, 3*self.out_channels:] + wh[:, 3*self.out_channels:] \n",
    "                        + wc[:, 2*self.out_channels: ]*c_t)\n",
    "        h_t = o_t * F.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class convRNN_1_layer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(convRNN_1_layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.convRNN = ConvLSTMCell(256,256,kernel_size=3, padding=1, stride=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256,6,kernel_size=1, padding=0, stride=1),\n",
    "            nn.AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hx):\n",
    "        x = self.features(x)\n",
    "        x, hx = self.convRNN(x, hx)\n",
    "        x = self.classifier(x).squeeze().unsqueeze(0)\n",
    "        return x, hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copyAlexNetParameters(model, target):\n",
    "    for i, a in enumerate(model.features):\n",
    "        if type(a) is torch.nn.modules.conv.Conv2d:\n",
    "            target.features[i].weight = a.weight\n",
    "            target.features[i].bias   = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FrameError(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return \"Frame Error number :\"+str(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trainSequence(model, dataset=\"/video/Gesture/\", criterion = nn.CrossEntropyLoss(), lr = 0.01):\n",
    "    videos = glob.glob(dataset+'*.mp4')\n",
    "    print(\"Nb video to handle : \", len(videos))\n",
    "    model.cuda()\n",
    "    \n",
    "    optimizer = optim.SGD( [{'params': model.convRNN.parameters(), \n",
    "                            'params': model.classifier.parameters()} ], \n",
    "                          lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(\"epoch:\", epoch)\n",
    "        random.shuffle(videos)\n",
    "        for video in videos:\n",
    "            rl = 0\n",
    "            model.train()\n",
    "            \n",
    "            print(\"Training on Video \", video)\n",
    "            fName = path.splitext(path.basename(video))[0] #basename\n",
    "            annotation = readFrameAnnotation(dataset+'annotation/'+fName) #read annotation\n",
    "\n",
    "            videoCap = cv2.VideoCapture(video)\n",
    "            nframe = 0\n",
    "            i = 0\n",
    "            for seq, gesture in annotation:\n",
    "                print(\"Sequence \", seq, \" Gesture : \", gesture)\n",
    "                while nframe != seq[0]:\n",
    "                    ret, frame = videoCap.read()\n",
    "                    if not ret:\n",
    "                        print(\"Error : None Frame \", nframe)\n",
    "                        exit(0)\n",
    "                    nframe += 1\n",
    "                    \n",
    "                    \n",
    "                rl += trainGesture(model, videoCap, seq[1]-seq[0], gesture, criterion, optimizer)\n",
    "                \n",
    "                i += 1\n",
    "                if i%5 == 4:\n",
    "                    print(\"[epoch %d] loss : %.3f\" % (epoch, rl/5) )\n",
    "                    #i = 0\n",
    "                    rl = 0.0\n",
    "            \n",
    "            if j%5 == 4:\n",
    "                r = testNet(model, rootDir+'test/')\n",
    "                if r > best:\n",
    "                    best = r\n",
    "                    print(\"Saving best model\")\n",
    "                    torch.save(model, 'best-model.ckpt')\n",
    "            #torch.save(model, path.join('model-'+str(epoch)+\".ckpt\"))\n",
    "            #lr = 0.001\n",
    "            j += 1\n",
    "            videoCap.release()\n",
    "        lr = 0.001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trainGesture(model, video, nframe, gesture, criterion, optimize, batchSize=32):\n",
    "    frame = 0\n",
    "    #define lstm hidden states\n",
    "    hx = Variable(torch.Tensor(1,256,6,6).random_().cuda())\n",
    "    cx = Variable(torch.Tensor(1,256,6,6).random_().cuda())\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "    \n",
    "    while frame != nframe:\n",
    "        #read frame\n",
    "        ret, vidframe = video.read()\n",
    "        frame += 1\n",
    "        if ret:\n",
    "            inputs = torch.Tensor(1,3,225,225).cuda()\n",
    "            inputs[0] = t(vidframe)\n",
    "            outputs = model(Variable(inputs), (hx, cx))\n",
    "\n",
    "            pred = outputs[0]\n",
    "\n",
    "            hx, cx = outputs[1]\n",
    "\n",
    "            label = torch.LongTensor([gesture]).cuda()\n",
    "            loss = criterion(pred, Variable(label))\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimize.step()\n",
    "            running_loss += loss.data[0]\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = convRNN_1_layer()\n",
    "copyAlexNetParameters(models.alexnet(pretrained=True), model)\n",
    "trainSequence(model, dataset=\"/video/Gesture/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
