{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network for Gesture Recognition from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import ModelDefinition\n",
    "from dataset import ReadImages, collection\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFrameAnnotation(annotationFile):\n",
    "    \"\"\"\n",
    "        read annotation file\n",
    "        return the list of annotation ([start, end], gesture)\n",
    "    \"\"\"\n",
    "    anno = []\n",
    "    for l in open(rootDir+'annotation/'+fName).read().splitlines():\n",
    "        s = l.split(' ')\n",
    "        anno += [ ([int(s[1]), int(s[2])], int(s[0]))]\n",
    "    return anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findGestureFrame(frameNumber, annotationFile):\n",
    "    \"\"\"\n",
    "        from Frame Number and the list of annotation\n",
    "        return the Gesture or None if not in annation\n",
    "    \"\"\"\n",
    "    for seq, gest in annotationFile:\n",
    "        if frameNumber >= seq[0] and frameNumber <= seq[1]:\n",
    "            return gest\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copyParameters(net, netBase):\n",
    "    for i, f in enumerate(net.features):\n",
    "        if type(f) is torch.nn.modules.conv.Conv2d:\n",
    "            f.weight.data = netBase.features[i].weight.data\n",
    "            f.bias.data = netBase.features[i].bias.data\n",
    "    for i, c in enumerate(net.classifier):\n",
    "        if type(c) is torch.nn.modules.linear.Linear:\n",
    "            if c.weight.size() == netBase.classifier[i].weight.size():\n",
    "                c.weight.data = netBase.classifier[i].weight.data\n",
    "                c.bias.data = netBase.classifier[i].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillInput(nframe, video):\n",
    "    t = transforms.Compose(\n",
    "                (transforms.ToPILImage(),\n",
    "                transforms.Scale(225),\n",
    "                transforms.RandomCrop(225),\n",
    "                transforms.ToTensor())\n",
    "                )\n",
    "    inputs = torch.Tensor(nframe,3,225,225)\n",
    "    for j in range(nframe):\n",
    "        ret, frame = video.read()\n",
    "        inputs[j] = t(frame)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO : test if difference between learning only gesture per batch\n",
    "\n",
    "def learnSequence(sequence, gesture, video, model, criterion, optimize, batchSize=32):\n",
    "    numberFrame = seq[1] - seq[0]\n",
    "    running_loss = 0\n",
    "    while numberFrame > 0:\n",
    "        if numberFrame >= batchSize:\n",
    "            inputs = fillInput(batchSize, video)\n",
    "            numberFrame -= batchSize\n",
    "            \n",
    "            labels = torch.LongTensor([gesture]*batchSize)\n",
    "        else:\n",
    "            inputs = fillInput(numberFrame, video)\n",
    "            labels = torch.LongTensor([gesture]*numberFrame)\n",
    "            numberFrame = 0\n",
    "        #inputs.cuda()\n",
    "        #labels.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, Variable(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video  /video/Gesture/fR01.mp4\n",
      "Sequence  [0, 47]\n",
      "1.53009283543\n",
      "Sequence  [54, 89]\n",
      "66.8847647905\n",
      "Sequence  [93, 141]\n",
      "4.48430073261\n",
      "Sequence  [158, 194]\n",
      "3.85082888603\n",
      "Sequence  [200, 247]\n",
      "3.89734107256\n",
      "Sequence  [258, 302]\n",
      "13.97313869\n",
      "Sequence  [307, 351]\n",
      "5.16192400455\n",
      "Sequence  [355, 393]\n",
      "8.56405293941\n",
      "Sequence  [398, 446]\n",
      "7.77778601646\n",
      "Sequence  [457, 503]\n",
      "12.2557734251\n",
      "Sequence  [508, 597]\n",
      "3.32094960846\n",
      "Sequence  [601, 647]\n",
      "23.1490075588\n",
      "Sequence  [650, 684]\n",
      "3.28633987904\n",
      "Sequence  [694, 733]\n",
      "5.03652656078\n",
      "Sequence  [736, 770]\n",
      "2.75052905083\n",
      "Sequence  [789, 827]\n",
      "11.6547495127\n",
      "Sequence  [830, 873]\n",
      "4.07329082489\n",
      "Sequence  [1015, 1056]\n",
      "3.27327257395\n",
      "Sequence  [1059, 1107]\n",
      "14.1854227781\n",
      "Sequence  [1112, 1156]\n",
      "4.09567332268\n",
      "Sequence  [1163, 1210]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a94bdd19d352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgesture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sequence \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearnSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgesture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideoCap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvideoCap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7ca9b8a43f27>\u001b[0m in \u001b[0;36mlearnSequence\u001b[0;34m(sequence, gesture, video, model, criterion, optimize, batchSize)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rootDir = '/video/Gesture/'\n",
    "model = models.AlexNet(num_classes=7)\n",
    "copyParameters(model, models.alexnet(pretrained=True))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for video in glob.glob(rootDir+'*.mp4'):\n",
    "    print(\"Video \", video)\n",
    "    \n",
    "    fName = path.splitext(path.basename(video))[0] #basename\n",
    "    annotation = readFrameAnnotation(rootDir+'annotation/'+fName) #read annotation\n",
    "    \n",
    "    videoCap = cv2.VideoCapture(video)\n",
    "    \n",
    "    for seq, gesture in annotation:\n",
    "        print(\"Sequence \", seq, \" Gesture : \", gesture)\n",
    "        rl = learnSequence(seq, gesture, videoCap, model, criterion, optimizer)\n",
    "        print(rl)\n",
    "videoCap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
